{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d2d22cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rijju\\Desktop\\mental-health-chatbot\\myenv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "GPU: NVIDIA GeForce RTX 3050 Laptop GPU\n",
      "Total samples: 53043\n",
      "Label counts:\n",
      " Normal        16351\n",
      "Depression    15404\n",
      "Suicidal      10653\n",
      "Anxiety        3888\n",
      "Bipolar        2877\n",
      "dtype: int64\n",
      "Train: 42434 | Val: 10609\n",
      "Class weight (inverse freq): {'Normal': 1.0, 'Depression': 1.05851755526658, 'Suicidal': 1.5082802547770702, 'Anxiety': 4.189128337085879, 'Bipolar': 5.670004353504571}\n",
      "pos_weight: [2.25813889503479, 2.4487972259521484, 3.914186477661133, 12.648761749267578, 17.473661422729492]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\rijju\\AppData\\Local\\Temp\\ipykernel_14596\\1128289565.py:159: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `FocalBCETrainer.__init__`. Use `processing_class` instead.\n",
      "  super().__init__(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7959' max='7959' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [7959/7959 1:08:07, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Micro F1</th>\n",
       "      <th>Macro F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.069400</td>\n",
       "      <td>0.060420</td>\n",
       "      <td>0.719104</td>\n",
       "      <td>0.684786</td>\n",
       "      <td>0.661325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.059200</td>\n",
       "      <td>0.053973</td>\n",
       "      <td>0.760513</td>\n",
       "      <td>0.746537</td>\n",
       "      <td>0.717881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.053200</td>\n",
       "      <td>0.063857</td>\n",
       "      <td>0.714825</td>\n",
       "      <td>0.717519</td>\n",
       "      <td>0.677255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.051600</td>\n",
       "      <td>0.050296</td>\n",
       "      <td>0.769867</td>\n",
       "      <td>0.753768</td>\n",
       "      <td>0.722028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.045400</td>\n",
       "      <td>0.049947</td>\n",
       "      <td>0.765058</td>\n",
       "      <td>0.761057</td>\n",
       "      <td>0.726836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.043600</td>\n",
       "      <td>0.050918</td>\n",
       "      <td>0.778223</td>\n",
       "      <td>0.764859</td>\n",
       "      <td>0.740126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.042500</td>\n",
       "      <td>0.051859</td>\n",
       "      <td>0.783892</td>\n",
       "      <td>0.764526</td>\n",
       "      <td>0.748610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.039700</td>\n",
       "      <td>0.044712</td>\n",
       "      <td>0.806920</td>\n",
       "      <td>0.798507</td>\n",
       "      <td>0.776322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.040800</td>\n",
       "      <td>0.048430</td>\n",
       "      <td>0.785237</td>\n",
       "      <td>0.781358</td>\n",
       "      <td>0.750683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.040400</td>\n",
       "      <td>0.046846</td>\n",
       "      <td>0.785140</td>\n",
       "      <td>0.776249</td>\n",
       "      <td>0.757470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.040000</td>\n",
       "      <td>0.050755</td>\n",
       "      <td>0.770985</td>\n",
       "      <td>0.768073</td>\n",
       "      <td>0.745688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.039200</td>\n",
       "      <td>0.044688</td>\n",
       "      <td>0.804628</td>\n",
       "      <td>0.797226</td>\n",
       "      <td>0.777642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.036900</td>\n",
       "      <td>0.043601</td>\n",
       "      <td>0.813455</td>\n",
       "      <td>0.806981</td>\n",
       "      <td>0.790461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.035700</td>\n",
       "      <td>0.041788</td>\n",
       "      <td>0.820932</td>\n",
       "      <td>0.813156</td>\n",
       "      <td>0.794231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.035600</td>\n",
       "      <td>0.043221</td>\n",
       "      <td>0.806783</td>\n",
       "      <td>0.808962</td>\n",
       "      <td>0.779433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>0.033400</td>\n",
       "      <td>0.043796</td>\n",
       "      <td>0.815274</td>\n",
       "      <td>0.803535</td>\n",
       "      <td>0.784523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>0.034000</td>\n",
       "      <td>0.041080</td>\n",
       "      <td>0.827306</td>\n",
       "      <td>0.819264</td>\n",
       "      <td>0.803186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>0.033900</td>\n",
       "      <td>0.042380</td>\n",
       "      <td>0.816658</td>\n",
       "      <td>0.811759</td>\n",
       "      <td>0.789895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>0.032200</td>\n",
       "      <td>0.044608</td>\n",
       "      <td>0.811212</td>\n",
       "      <td>0.809934</td>\n",
       "      <td>0.790272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.029900</td>\n",
       "      <td>0.045060</td>\n",
       "      <td>0.824531</td>\n",
       "      <td>0.820471</td>\n",
       "      <td>0.796305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4200</td>\n",
       "      <td>0.028000</td>\n",
       "      <td>0.045017</td>\n",
       "      <td>0.825907</td>\n",
       "      <td>0.822074</td>\n",
       "      <td>0.803563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4400</td>\n",
       "      <td>0.026200</td>\n",
       "      <td>0.044999</td>\n",
       "      <td>0.823180</td>\n",
       "      <td>0.818374</td>\n",
       "      <td>0.798002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4600</td>\n",
       "      <td>0.026100</td>\n",
       "      <td>0.041631</td>\n",
       "      <td>0.828261</td>\n",
       "      <td>0.825131</td>\n",
       "      <td>0.809407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4800</td>\n",
       "      <td>0.027100</td>\n",
       "      <td>0.046078</td>\n",
       "      <td>0.821565</td>\n",
       "      <td>0.814743</td>\n",
       "      <td>0.799604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.029200</td>\n",
       "      <td>0.042560</td>\n",
       "      <td>0.828828</td>\n",
       "      <td>0.821008</td>\n",
       "      <td>0.809501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5200</td>\n",
       "      <td>0.027600</td>\n",
       "      <td>0.043381</td>\n",
       "      <td>0.822104</td>\n",
       "      <td>0.820662</td>\n",
       "      <td>0.802526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5400</td>\n",
       "      <td>0.026400</td>\n",
       "      <td>0.045626</td>\n",
       "      <td>0.816066</td>\n",
       "      <td>0.811933</td>\n",
       "      <td>0.797059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5600</td>\n",
       "      <td>0.027100</td>\n",
       "      <td>0.041909</td>\n",
       "      <td>0.830671</td>\n",
       "      <td>0.829494</td>\n",
       "      <td>0.813272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5800</td>\n",
       "      <td>0.027400</td>\n",
       "      <td>0.046942</td>\n",
       "      <td>0.807533</td>\n",
       "      <td>0.805097</td>\n",
       "      <td>0.789707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.024600</td>\n",
       "      <td>0.041444</td>\n",
       "      <td>0.830682</td>\n",
       "      <td>0.827996</td>\n",
       "      <td>0.813460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6200</td>\n",
       "      <td>0.026400</td>\n",
       "      <td>0.044358</td>\n",
       "      <td>0.827738</td>\n",
       "      <td>0.822180</td>\n",
       "      <td>0.810255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6400</td>\n",
       "      <td>0.027500</td>\n",
       "      <td>0.041809</td>\n",
       "      <td>0.827744</td>\n",
       "      <td>0.826357</td>\n",
       "      <td>0.811104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6600</td>\n",
       "      <td>0.025600</td>\n",
       "      <td>0.043050</td>\n",
       "      <td>0.828476</td>\n",
       "      <td>0.825526</td>\n",
       "      <td>0.811010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6800</td>\n",
       "      <td>0.025300</td>\n",
       "      <td>0.043803</td>\n",
       "      <td>0.826855</td>\n",
       "      <td>0.823721</td>\n",
       "      <td>0.808842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.023500</td>\n",
       "      <td>0.042087</td>\n",
       "      <td>0.834035</td>\n",
       "      <td>0.831727</td>\n",
       "      <td>0.817702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7200</td>\n",
       "      <td>0.023100</td>\n",
       "      <td>0.044745</td>\n",
       "      <td>0.831311</td>\n",
       "      <td>0.827855</td>\n",
       "      <td>0.814309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7400</td>\n",
       "      <td>0.025500</td>\n",
       "      <td>0.043483</td>\n",
       "      <td>0.832582</td>\n",
       "      <td>0.830157</td>\n",
       "      <td>0.816100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7600</td>\n",
       "      <td>0.023400</td>\n",
       "      <td>0.043010</td>\n",
       "      <td>0.833916</td>\n",
       "      <td>0.831550</td>\n",
       "      <td>0.818079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7800</td>\n",
       "      <td>0.023200</td>\n",
       "      <td>0.043490</td>\n",
       "      <td>0.830414</td>\n",
       "      <td>0.829972</td>\n",
       "      <td>0.815722</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train result: TrainOutput(global_step=7959, training_loss=0.034278378008537995, metrics={'train_runtime': 4088.3262, 'train_samples_per_second': 31.138, 'train_steps_per_second': 1.947, 'total_flos': 8373866442988032.0, 'train_loss': 0.034278378008537995, 'epoch': 3.0})\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold optimization class weights: {'Normal': 0.07448273324169248, 'Depression': 0.0788412807005688, 'Suicidal': 0.1123408358702681, 'Anxiety': 0.31201772844630604, 'Bipolar': 0.4223174217411646}\n",
      "Selected global thresholds (weighted macro-F1): {'Normal': 0.35, 'Depression': 0.35, 'Suicidal': 0.44999999999999996, 'Anxiety': 0.65, 'Bipolar': 0.75}\n",
      "Saved model and tokenizer to: ../results/models/distress_model\n",
      "\n",
      "Text: I had a good day at work, everything went smoothly.\n",
      "Probabilities: [0.589 0.069 0.045 0.207 0.182]\n",
      "Predicted: {'Normal': 1, 'Depression': 0, 'Suicidal': 0, 'Anxiety': 0, 'Bipolar': 0}\n",
      "\n",
      "Text: I can’t get out of bed, I just want to sleep all day.\n",
      "Probabilities: [0.099 0.459 0.292 0.11  0.068]\n",
      "Predicted: {'Normal': 0, 'Depression': 1, 'Suicidal': 0, 'Anxiety': 0, 'Bipolar': 0}\n",
      "\n",
      "Text: Life is pointless and I don't want to continue.\n",
      "Probabilities: [0.057 0.499 0.499 0.064 0.036]\n",
      "Predicted: {'Normal': 0, 'Depression': 1, 'Suicidal': 1, 'Anxiety': 0, 'Bipolar': 0}\n",
      "\n",
      "Text: I swing between extreme highs and crushing lows.\n",
      "Probabilities: [0.124 0.195 0.167 0.145 0.561]\n",
      "Predicted: {'Normal': 0, 'Depression': 0, 'Suicidal': 0, 'Anxiety': 0, 'Bipolar': 1}\n",
      "\n",
      "Text: My heart races and I panic in crowded places.\n",
      "Probabilities: [0.183 0.122 0.081 0.458 0.128]\n",
      "Predicted: {'Normal': 0, 'Depression': 0, 'Suicidal': 0, 'Anxiety': 1, 'Bipolar': 0}\n",
      "\n",
      "Text: I feel mostly fine but worried before presentations.\n",
      "Probabilities: [0.072 0.052 0.039 0.897 0.061]\n",
      "Predicted: {'Normal': 0, 'Depression': 0, 'Suicidal': 0, 'Anxiety': 1, 'Bipolar': 0}\n",
      "\n",
      "Text: Sometimes I'm very energetic and talk too much, then crash.\n",
      "Probabilities: [0.309 0.097 0.075 0.244 0.336]\n",
      "Predicted: {'Normal': 0, 'Depression': 0, 'Suicidal': 0, 'Anxiety': 0, 'Bipolar': 1}\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# phase2_distress_detection_improved.py\n",
    "# =========================================================\n",
    "# Phase 2 – Distress Detection (Multi-label, CUDA-ready, improved for minority classes)\n",
    "# =========================================================\n",
    "\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "from itertools import product\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score\n",
    "\n",
    "from transformers import (\n",
    "    RobertaTokenizer,\n",
    "    RobertaConfig,\n",
    "    RobertaForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    set_seed,\n",
    ")\n",
    "\n",
    "# --------------------------\n",
    "# 0) Reproducibility & Device\n",
    "# --------------------------\n",
    "SEED = 42\n",
    "set_seed(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "if device.type == \"cuda\":\n",
    "    try:\n",
    "        print(\"GPU:\", torch.cuda.get_device_name(0))\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "# --------------------------\n",
    "# 1) Paths & Config\n",
    "# --------------------------\n",
    "DATA_PATH = \"../data/dataset1_conditions/processed/multilabled_clean.csv\"\n",
    "OUTPUT_DIR = \"../results/models/distress_model\"\n",
    "LOG_DIR = \"../results/logs\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "os.makedirs(LOG_DIR, exist_ok=True)\n",
    "\n",
    "TEXT_COL = \"cleaned_text\"\n",
    "LABEL_COLS = [\"Normal\", \"Depression\", \"Suicidal\", \"Anxiety\", \"Bipolar\"]\n",
    "MODEL_NAME = \"roberta-base\"\n",
    "MAX_LEN = 128\n",
    "TEST_SIZE = 0.2\n",
    "\n",
    "# --------------------------\n",
    "# 2) Load data & quick checks\n",
    "# --------------------------\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "missing_cols = [c for c in [TEXT_COL] + LABEL_COLS if c not in df.columns]\n",
    "if missing_cols:\n",
    "    raise ValueError(f\"Missing columns in CSV: {missing_cols}\")\n",
    "\n",
    "df[TEXT_COL] = df[TEXT_COL].astype(str)\n",
    "for c in LABEL_COLS:\n",
    "    df[c] = df[c].fillna(0).astype(int)\n",
    "\n",
    "texts = df[TEXT_COL].tolist()\n",
    "labels = df[LABEL_COLS].values  # (N,5)\n",
    "\n",
    "print(\"Total samples:\", len(texts))\n",
    "print(\"Label counts:\\n\", df[LABEL_COLS].sum())\n",
    "\n",
    "# --------------------------\n",
    "# 3) Train / Val split\n",
    "# --------------------------\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    texts, labels, test_size=TEST_SIZE, random_state=SEED\n",
    ")\n",
    "print(f\"Train: {len(X_train)} | Val: {len(X_val)}\")\n",
    "\n",
    "# --------------------------\n",
    "# 4) Tokenizer & encodings\n",
    "# --------------------------\n",
    "tokenizer = RobertaTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "train_enc = tokenizer(X_train, truncation=True, padding=True, max_length=MAX_LEN)\n",
    "val_enc   = tokenizer(X_val,   truncation=True, padding=True, max_length=MAX_LEN)\n",
    "\n",
    "# --------------------------\n",
    "# 5) PyTorch Dataset\n",
    "# --------------------------\n",
    "import torch.utils.data as tud\n",
    "\n",
    "class DistressDataset(tud.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {k: torch.tensor(v[idx]) for k, v in self.encodings.items()}\n",
    "        item[\"labels\"] = torch.tensor(self.labels[idx], dtype=torch.float)\n",
    "        return item\n",
    "\n",
    "train_ds = DistressDataset(train_enc, y_train)\n",
    "val_ds   = DistressDataset(val_enc,   y_val)\n",
    "\n",
    "# --------------------------\n",
    "# 6) Sample weighting (oversample minority examples)\n",
    "#    We compute per-sample weights based on inverse label frequency.\n",
    "# --------------------------\n",
    "label_counts = y_train.sum(axis=0)  # per-class counts on train\n",
    "label_counts = np.maximum(label_counts, 1.0)  # avoid zeros\n",
    "\n",
    "# class weight: inverse frequency (so rare classes get larger weight)\n",
    "class_weight = (label_counts.max() / label_counts).astype(float)\n",
    "print(\"Class weight (inverse freq):\", dict(zip(LABEL_COLS, class_weight.tolist())))\n",
    "\n",
    "# per-sample weight: sum of class_weight for the positive labels the sample has\n",
    "sample_weights = (y_train * class_weight).sum(axis=1) + 1.0  # +1 to keep baseline\n",
    "sample_weights = sample_weights.astype(float)\n",
    "\n",
    "from torch.utils.data import WeightedRandomSampler\n",
    "train_sampler = WeightedRandomSampler(weights=sample_weights, num_samples=len(sample_weights), replacement=True)\n",
    "\n",
    "# --------------------------\n",
    "# 7) pos_weight for BCEWithLogitsLoss (torch expects tensor)\n",
    "#    pos_weight = (N - pos) / pos  (per-class)\n",
    "# --------------------------\n",
    "labels_tensor = torch.tensor(y_train, dtype=torch.float)\n",
    "pos = labels_tensor.sum(dim=0)\n",
    "neg = labels_tensor.shape[0] - pos\n",
    "pos_ = torch.where(pos == 0, torch.ones_like(pos), pos)\n",
    "pos_weight = (neg / pos_).to(device)\n",
    "print(\"pos_weight:\", pos_weight.tolist())\n",
    "\n",
    "# --------------------------\n",
    "# 8) Model setup\n",
    "# --------------------------\n",
    "num_labels = len(LABEL_COLS)\n",
    "config = RobertaConfig.from_pretrained(MODEL_NAME, num_labels=num_labels, problem_type=\"multi_label_classification\")\n",
    "model = RobertaForSequenceClassification.from_pretrained(MODEL_NAME, config=config)\n",
    "model.to(device)\n",
    "\n",
    "# --------------------------\n",
    "# 9) Custom Trainer: focal + pos_weight + custom train dataloader (sampler)\n",
    "# --------------------------\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import BCEWithLogitsLoss\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "class FocalBCETrainer(Trainer):\n",
    "    def __init__(self, *args, train_sampler=None, pos_weight=None, focal_gamma=2.0, focal_alpha=1.0, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.train_sampler = train_sampler\n",
    "        self.pos_weight = pos_weight\n",
    "        self.focal_gamma = focal_gamma\n",
    "        self.focal_alpha = focal_alpha\n",
    "\n",
    "    def get_train_dataloader(self):\n",
    "        if self.train_dataset is None:\n",
    "            return None\n",
    "        # make DataLoader that uses our sampler\n",
    "        return DataLoader(\n",
    "            self.train_dataset,\n",
    "            batch_size=self.args.train_batch_size,\n",
    "            sampler=self.train_sampler,\n",
    "            collate_fn=self.data_collator,\n",
    "            drop_last=self.args.dataloader_drop_last,\n",
    "            num_workers=self.args.dataloader_num_workers if hasattr(self.args, \"dataloader_num_workers\") else 0,\n",
    "            pin_memory=self.args.dataloader_pin_memory,\n",
    "        )\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        labels = inputs.get(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get(\"logits\")\n",
    "\n",
    "        bce_loss = BCEWithLogitsLoss(reduction=\"none\")\n",
    "        loss = bce_loss(logits, labels.float())\n",
    "\n",
    "        # Focal scaling\n",
    "        probas = torch.sigmoid(logits)\n",
    "        pt = torch.where(labels == 1, probas, 1 - probas)\n",
    "        focal_factor = (1 - pt) ** self.focal_gamma\n",
    "        loss = focal_factor * loss\n",
    "\n",
    "        return (loss.mean(), outputs) if return_outputs else loss.mean()\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# 10) Metrics -- dynamic threshold tuning will be run after training.\n",
    "# --------------------------\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    probs = 1 / (1 + np.exp(-logits))\n",
    "    preds = (probs >= 0.5).astype(int)\n",
    "    return {\n",
    "        \"micro_f1\": f1_score(labels, preds, average=\"micro\", zero_division=0),\n",
    "        \"macro_f1\": f1_score(labels, preds, average=\"macro\", zero_division=0),\n",
    "        \"accuracy\": accuracy_score(labels, preds),\n",
    "    }\n",
    "\n",
    "# --------------------------\n",
    "# 11) TrainingArguments (use fp16 on CUDA)\n",
    "# --------------------------\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=32,\n",
    "    gradient_accumulation_steps=2,   # adjust to taste\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=200,\n",
    "    logging_steps=100,\n",
    "    save_steps=200,\n",
    "    save_total_limit=2,\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"macro_f1\",\n",
    "    greater_is_better=True,\n",
    "    fp16=(device.type == \"cuda\"),\n",
    "    dataloader_pin_memory=True,\n",
    "    report_to=\"none\",\n",
    "    logging_dir=LOG_DIR,\n",
    ")\n",
    "\n",
    "# --------------------------\n",
    "# 12) Create trainer and train\n",
    "# --------------------------\n",
    "trainer = FocalBCETrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_sampler=train_sampler,\n",
    "    pos_weight=pos_weight,\n",
    "    focal_gamma=2.0,\n",
    "    focal_alpha=1.0,\n",
    ")\n",
    "\n",
    "train_result = trainer.train()\n",
    "print(\"Train result:\", train_result)\n",
    "\n",
    "# --------------------------\n",
    "# 13) Evaluate on validation and do threshold search (weighted macro-F1)\n",
    "# --------------------------\n",
    "pred_out = trainer.predict(val_ds)  # returns PredictionOutput\n",
    "logits_val = pred_out.predictions  # (N_val, C)\n",
    "labels_val = pred_out.label_ids\n",
    "\n",
    "probs_val = 1 / (1 + np.exp(-logits_val))\n",
    "\n",
    "# weighted macro-F1 search: give more weight to rare classes\n",
    "train_label_counts = y_train.sum(axis=0).astype(float)\n",
    "class_weights_for_thresh = (1.0 / (train_label_counts + 1e-9))\n",
    "class_weights_for_thresh = class_weights_for_thresh / class_weights_for_thresh.sum()  # normalized\n",
    "print(\"Threshold optimization class weights:\", dict(zip(LABEL_COLS, class_weights_for_thresh.tolist())))\n",
    "\n",
    "# grid to search\n",
    "grid_vals = np.linspace(0.35, 0.85, 6)  # 6^5 = 7776 combos -> reasonable\n",
    "best_score = -1.0\n",
    "best_thresh = None\n",
    "\n",
    "# For speed: generate all combinations and iterate\n",
    "for combo in product(grid_vals, repeat=len(LABEL_COLS)):\n",
    "    thresh = np.array(combo)\n",
    "    preds = (probs_val >= thresh).astype(int)\n",
    "    # if none predicted for sample, allow top-1 if top prob >= 0.3 (prevents empty set)\n",
    "    none_mask = preds.sum(axis=1) == 0\n",
    "    if none_mask.any():\n",
    "        top_idx = probs_val[none_mask].argmax(axis=1)\n",
    "        # set the top prediction to 1 when its prob >= 0.3\n",
    "        top_probs = probs_val[none_mask, :].max(axis=1)\n",
    "        pick_mask = top_probs >= 0.3\n",
    "        rows_idx = np.where(none_mask)[0][pick_mask]\n",
    "        cols_idx = probs_val[none_mask][:, :].argmax(axis=1)[pick_mask]\n",
    "        preds[rows_idx, cols_idx] = 1\n",
    "\n",
    "    # compute per-class f1\n",
    "    f1s = np.array([f1_score(labels_val[:, i], preds[:, i], zero_division=0) for i in range(len(LABEL_COLS))])\n",
    "    weighted_macro = float(np.sum(class_weights_for_thresh * f1s))\n",
    "    if weighted_macro > best_score:\n",
    "        best_score = weighted_macro\n",
    "        best_thresh = thresh.copy()\n",
    "\n",
    "best_thresholds = {LABEL_COLS[i]: float(best_thresh[i]) for i in range(len(LABEL_COLS))}\n",
    "print(\"Selected global thresholds (weighted macro-F1):\", best_thresholds)\n",
    "\n",
    "# Optionally apply exclusivity: if any distress label flagged, remove 'Normal'\n",
    "def apply_post_rules(preds_np):\n",
    "    # preds_np: (n, C)\n",
    "    preds_np = preds_np.copy()\n",
    "    distress_idx = [i for i, lab in enumerate(LABEL_COLS) if lab != \"Normal\"]\n",
    "    # if any distress predicted, set Normal=0\n",
    "    any_distress = preds_np[:, distress_idx].sum(axis=1) > 0\n",
    "    preds_np[any_distress, 0] = 0\n",
    "    return preds_np\n",
    "\n",
    "# Save thresholds\n",
    "with open(os.path.join(OUTPUT_DIR, \"thresholds.json\"), \"w\") as f:\n",
    "    json.dump(best_thresholds, f, indent=2)\n",
    "\n",
    "# --------------------------\n",
    "# 14) Save model + tokenizer (produces pytorch_model.bin & config.json)\n",
    "# --------------------------\n",
    "trainer.save_model(OUTPUT_DIR)\n",
    "tokenizer.save_pretrained(OUTPUT_DIR)\n",
    "print(\"Saved model and tokenizer to:\", OUTPUT_DIR)\n",
    "\n",
    "# --------------------------\n",
    "# 15) Quick inference sanity checks (use saved thresholds & rules)\n",
    "# --------------------------\n",
    "model.eval()\n",
    "sample_texts = [\n",
    "    \"I had a good day at work, everything went smoothly.\",\n",
    "    \"I can’t get out of bed, I just want to sleep all day.\",\n",
    "    \"Life is pointless and I don't want to continue.\",\n",
    "    \"I swing between extreme highs and crushing lows.\",\n",
    "    \"My heart races and I panic in crowded places.\",\n",
    "    \"I feel mostly fine but worried before presentations.\",\n",
    "    \"Sometimes I'm very energetic and talk too much, then crash.\",\n",
    "]\n",
    "\n",
    "enc = tokenizer(sample_texts, truncation=True, padding=True, max_length=MAX_LEN, return_tensors=\"pt\").to(device)\n",
    "with torch.no_grad():\n",
    "    logits = model(**enc).logits\n",
    "    probs = torch.sigmoid(logits).cpu().numpy()\n",
    "\n",
    "thresh_arr = np.array([best_thresholds[lab] for lab in LABEL_COLS])\n",
    "\n",
    "for t, p in zip(sample_texts, probs):\n",
    "    pred = (p >= thresh_arr).astype(int)\n",
    "    # if none predicted, pick top-1 if prob>=0.3\n",
    "    if pred.sum() == 0:\n",
    "        top_i = int(p.argmax())\n",
    "        if p[top_i] >= 0.3:\n",
    "            pred[top_i] = 1\n",
    "    pred = apply_post_rules(pred.reshape(1, -1))[0]\n",
    "    print(\"\\nText:\", t)\n",
    "    print(\"Probabilities:\", np.round(p, 3))\n",
    "    print(\"Predicted:\", dict(zip(LABEL_COLS, pred.tolist())))\n",
    "\n",
    "# --------------------------\n",
    "# 16) Optionally save thresholds & a small calibration file\n",
    "# --------------------------\n",
    "# (we already saved thresholds.json)\n",
    "if device.type == \"cuda\":\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "print(\"Done.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0df5e69f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved idx2condition.json at ../results/models/distress_model/idx2condition.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "# Order must match the one used during training!\n",
    "conditions = [\"Normal\", \"Depression\", \"Suicidal\", \"Anxiety\", \"Bipolar\"]\n",
    "\n",
    "idx2condition = {i: cond for i, cond in enumerate(conditions)}\n",
    "\n",
    "output_path = \"../results/models/distress_model/idx2condition.json\"\n",
    "os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "\n",
    "with open(output_path, \"w\") as f:\n",
    "    json.dump(idx2condition, f, indent=4)\n",
    "\n",
    "print(f\"Saved idx2condition.json at {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3bf91b5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Text: I feel like I can’t handle life, and my chest is always tight with worry.\n",
      "Probabilities: [0.064 0.207 0.087 0.561 0.054]\n",
      "Predicted: {'Normal': 0, 'Depression': 0, 'Suicidal': 0, 'Anxiety': 1, 'Bipolar': 0}\n",
      "\n",
      "Text: I can’t sleep because my thoughts won’t stop, and everything feels meaningless.\n",
      "Probabilities: [0.149 0.517 0.249 0.172 0.086]\n",
      "Predicted: {'Normal': 0, 'Depression': 1, 'Suicidal': 0, 'Anxiety': 0, 'Bipolar': 0}\n",
      "\n",
      "Text: I feel so worthless, I don’t think I should keep going.\n",
      "Probabilities: [0.079 0.468 0.494 0.06  0.038]\n",
      "Predicted: {'Normal': 0, 'Depression': 1, 'Suicidal': 1, 'Anxiety': 0, 'Bipolar': 0}\n",
      "\n",
      "Text: Every day hurts, and sometimes I wish it would all just end.\n",
      "Probabilities: [0.055 0.432 0.51  0.06  0.04 ]\n",
      "Predicted: {'Normal': 0, 'Depression': 1, 'Suicidal': 1, 'Anxiety': 0, 'Bipolar': 0}\n",
      "\n",
      "Text: When I’m on a high, I talk too much, but then I crash and panic about everything.\n",
      "Probabilities: [0.137 0.167 0.089 0.334 0.298]\n",
      "Predicted: {'Normal': 0, 'Depression': 0, 'Suicidal': 0, 'Anxiety': 1, 'Bipolar': 0}\n",
      "\n",
      "Text: I swing between excitement and fear all the time.\n",
      "Probabilities: [0.342 0.133 0.092 0.279 0.215]\n",
      "Predicted: {'Normal': 1, 'Depression': 0, 'Suicidal': 0, 'Anxiety': 0, 'Bipolar': 0}\n",
      "\n",
      "Text: I feel mostly fine, but I get nervous before important meetings.\n",
      "Probabilities: [0.155 0.071 0.045 0.518 0.191]\n",
      "Predicted: {'Normal': 0, 'Depression': 0, 'Suicidal': 0, 'Anxiety': 1, 'Bipolar': 0}\n",
      "\n",
      "Text: Life is okay, but I can’t stop worrying at night.\n",
      "Probabilities: [0.09  0.288 0.158 0.484 0.058]\n",
      "Predicted: {'Normal': 0, 'Depression': 0, 'Suicidal': 0, 'Anxiety': 1, 'Bipolar': 0}\n",
      "\n",
      "Text: After my highs, I always fall into such a deep, dark place.\n",
      "Probabilities: [0.088 0.304 0.161 0.075 0.38 ]\n",
      "Predicted: {'Normal': 0, 'Depression': 0, 'Suicidal': 0, 'Anxiety': 0, 'Bipolar': 1}\n",
      "\n",
      "Text: My mood swings take me from extreme energy to crushing sadness.\n",
      "Probabilities: [0.169 0.179 0.116 0.219 0.454]\n",
      "Predicted: {'Normal': 0, 'Depression': 0, 'Suicidal': 0, 'Anxiety': 0, 'Bipolar': 1}\n"
     ]
    }
   ],
   "source": [
    "sample_texts = [\n",
    "    \"I feel like I can’t handle life, and my chest is always tight with worry.\",\n",
    "    \"I can’t sleep because my thoughts won’t stop, and everything feels meaningless.\",\n",
    "    \"I feel so worthless, I don’t think I should keep going.\",\n",
    "    \"Every day hurts, and sometimes I wish it would all just end.\",\n",
    "    \"When I’m on a high, I talk too much, but then I crash and panic about everything.\",\n",
    "\n",
    "    \"I swing between excitement and fear all the time.\",\n",
    "    \"I feel mostly fine, but I get nervous before important meetings.\",\n",
    "\n",
    "\"Life is okay, but I can’t stop worrying at night.\",\n",
    "\"After my highs, I always fall into such a deep, dark place.\",\n",
    "\n",
    "\"My mood swings take me from extreme energy to crushing sadness.\"\n",
    "\n",
    "]\n",
    "enc = tokenizer(sample_texts, truncation=True, padding=True, max_length=MAX_LEN, return_tensors=\"pt\").to(device)\n",
    "with torch.no_grad():\n",
    "    logits = model(**enc).logits\n",
    "    probs = torch.sigmoid(logits).cpu().numpy()\n",
    "\n",
    "thresh_arr = np.array([best_thresholds[lab] for lab in LABEL_COLS])\n",
    "\n",
    "for t, p in zip(sample_texts, probs):\n",
    "    pred = (p >= thresh_arr).astype(int)\n",
    "    # if none predicted, pick top-1 if prob>=0.3\n",
    "    if pred.sum() == 0:\n",
    "        top_i = int(p.argmax())\n",
    "        if p[top_i] >= 0.3:\n",
    "            pred[top_i] = 1\n",
    "    pred = apply_post_rules(pred.reshape(1, -1))[0]\n",
    "    print(\"\\nText:\", t)\n",
    "    print(\"Probabilities:\", np.round(p, 3))\n",
    "    print(\"Predicted:\", dict(zip(LABEL_COLS, pred.tolist())))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7961e5bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d3796c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ba8598",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
